{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW_pcY-vXQ1U"
      },
      "source": [
        "### Upload and Extract a Zip File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "t6RR2gELvupV",
        "outputId": "517b6993-63e8-45ec-b678-f1fcbf391d5a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract a ZIP file\n",
        "with zipfile.ZipFile('Khaleej-2004.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('destination_folder')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maaw12-nXrYJ"
      },
      "source": [
        "### Extract and List the Contents of a Zip File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbdWJeThGd3n",
        "outputId": "9cdb3d64-edc4-4a61-9004-aef9c6601fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files have been extracted to: 'C:\\Users\\shath\\Downloads\\Khaleej-2004'\n",
            "Number of extracted files: 2\n",
            "First 5 files: ['Khaleej-2004', 'Khaleej-2004.zip']\n",
            "Subdirectory: Khaleej-2004\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Specify the path to the ZIP file and the extraction directory\n",
        "zip_file_path = r\"C:\\Users\\shath\\Downloads\\Khaleej-2004.zip\"  # Update this to the full path of your ZIP file\n",
        "extract_dir = r\"C:\\Users\\shath\\Downloads\\Khaleej-2004\"  # Update this to the desired extraction folder\n",
        "\n",
        "try:\n",
        "    # Check if the ZIP file exists\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        print(f\"File '{zip_file_path}' does not exist.\")\n",
        "    else:\n",
        "        # Extract the contents of the ZIP file\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            # Create the extraction directory if it doesn't exist\n",
        "            os.makedirs(extract_dir, exist_ok=True)\n",
        "            zip_ref.extractall(extract_dir)\n",
        "            print(f\"Files have been extracted to: '{extract_dir}'\")\n",
        "\n",
        "        # List the extracted files\n",
        "        extracted_files = os.listdir(extract_dir)\n",
        "        print(f\"Number of extracted files: {len(extracted_files)}\")\n",
        "        if extracted_files:\n",
        "            print(\"First 5 files:\", extracted_files[:5])\n",
        "        else:\n",
        "            print(\"No files were extracted. Please check the contents of the ZIP file.\")\n",
        "\n",
        "        # Check for subdirectories\n",
        "        for folder in extracted_files:\n",
        "            folder_path = os.path.join(extract_dir, folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                print(f\"Subdirectory: {folder}\")\n",
        "\n",
        "except zipfile.BadZipFile:\n",
        "    print(f\"The file '{zip_file_path}' is corrupted or not a valid ZIP file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvUhku7edVX5",
        "outputId": "e03bfe3d-ad5d-48b2-f2ac-b5d6d23c2ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Categories: 4\n",
            "Category: Economy\n",
            "Category: International news\n",
            "Category: Local News\n",
            "Category: Sports\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to the directory containing the categories\n",
        "directory_path = r\"C:\\Users\\shath\\Downloads\\Khaleej-2004\\Khaleej-2004\"  # Update this path if needed\n",
        "\n",
        "try:\n",
        "    # List all folders in the directory\n",
        "    categories = [folder for folder in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, folder))]\n",
        "    \n",
        "    # Print the categories\n",
        "    print(f\"Number of Categories: {len(categories)}\")\n",
        "    for category in categories:\n",
        "        print(f\"Category: {category}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"The directory '{directory_path}' does not exist.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGtX4diCX49M"
      },
      "source": [
        "### Load and Count Files in Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAM1gwT6HWUe",
        "outputId": "221220e7-4a01-465e-f861-4b77dd790a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "إجمالي عدد الملفات: 5690\n",
            "عدد الملفات حسب الفئة:\n",
            "الفئة: Economy, عدد الملفات: 909\n",
            "أول 5 ملفات في الفئة 'Economy': ['C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Economy\\\\arc_Articlesww0221.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Economy\\\\arc_Articlesww0313.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Economy\\\\arc_Articlesww03de.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Economy\\\\arc_Articlesww04a5.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Economy\\\\arc_Articlesww0521.html']\n",
            "الفئة: International news, عدد الملفات: 953\n",
            "أول 5 ملفات في الفئة 'International news': ['C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\International news\\\\arc_Articlesww02c5.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\International news\\\\arc_Articlesww05b8.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\International news\\\\arc_Articlesww0621.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\International news\\\\arc_Articlesww085f.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\International news\\\\arc_Articlesww0b37.html']\n",
            "الفئة: Local News, عدد الملفات: 2398\n",
            "أول 5 ملفات في الفئة 'Local News': ['C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Local News\\\\arc_Articlesww0054.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Local News\\\\arc_Articlesww01e3.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Local News\\\\arc_Articlesww0234.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Local News\\\\arc_Articlesww0318.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Local News\\\\arc_Articlesww03fe.html']\n",
            "الفئة: Sports, عدد الملفات: 1430\n",
            "أول 5 ملفات في الفئة 'Sports': ['C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Sports\\\\arc_Articlesww0043.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Sports\\\\arc_Articlesww0177.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Sports\\\\arc_Articlesww0406.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Sports\\\\arc_Articlesww04c9.html', 'C:\\\\Users\\\\shath\\\\Downloads\\\\Khaleej-2004\\\\Khaleej-2004\\\\Sports\\\\arc_Articlesww04f7.html']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob  # تأكد من استيراد glob\n",
        "\n",
        "# التحقق من وجود المجلد\n",
        "if not os.path.exists(directory_path):\n",
        "    print(f\"المجلد '{directory_path}' غير موجود.\")\n",
        "else:\n",
        "    # تحميل جميع الملفات النصية\n",
        "    categories = {}\n",
        "    total_files = 0\n",
        "\n",
        "    for folder in os.listdir(directory_path):\n",
        "        folder_path = os.path.join(directory_path, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # البحث عن ملفات HTML فقط\n",
        "            html_files = glob.glob(os.path.join(folder_path, \"*.html\"))\n",
        "            if html_files:  # التأكد من وجود ملفات\n",
        "                categories[folder] = html_files\n",
        "                total_files += len(html_files)\n",
        "\n",
        "    # عرض عدد الملفات لكل فئة\n",
        "    print(f\"إجمالي عدد الملفات: {total_files}\")\n",
        "    print(\"عدد الملفات حسب الفئة:\")\n",
        "    for category, files in categories.items():\n",
        "        print(f\"الفئة: {category}, عدد الملفات: {len(files)}\")\n",
        "        print(f\"أول 5 ملفات في الفئة '{category}': {files[:5]}\")  # عرض أول 5 ملفات\n",
        "\n",
        "    # معالجة حالة عدم وجود ملفات HTML\n",
        "    if total_files == 0:\n",
        "        print(\"لم يتم العثور على أي ملفات HTML في المجلدات.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3yHveNtMWlRX"
      },
      "outputs": [],
      "source": [
        "categories = {}\n",
        "for folder in os.listdir(directory_path):\n",
        "    folder_path = os.path.join(directory_path, folder)\n",
        "    # استبعاد المجلد الجذر Khaleej-2004\n",
        "    if os.path.isdir(folder_path) and folder != 'Khaleej-2004':\n",
        "        categories[folder] = []\n",
        "        for file_path in os.listdir(folder_path):\n",
        "            full_path = os.path.join(folder_path, file_path)\n",
        "            if os.path.isfile(full_path):\n",
        "                with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                    categories[folder].append(f.read())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBfizwk4ZwTp"
      },
      "source": [
        "### Read and Preview a Sample Document from Each Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7t5JkiNVIrvJ",
        "outputId": "7a96bcdf-ae70-4742-b99e-bbd8dd6fdc19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Category: Economy\n",
            "Error reading the file in category 'Economy': [Errno 2] No such file or directory: '  جمعية الاقتصاديين الدور المأمول   حسنا فعلت جمعية الاقتصاديين البحرينية بتنظيم المؤتمر الاقتصادي الأول والذي يركز على الرؤية المستقبلية للاقتصاد الخليجي برمته وليس على اقتصاد البحرين فحسب ان اختيار موضوع المؤتمر واتجاهه الاستراتيجي يعبر عن آفاق ايجابية مشكورة لدى الجمعية حيث انها منذ نشأتها عام لم تتول مسئولية عمل بهذا الحجم والاتجاه   ان تركيز المؤتمر على مستقبل الاقتصاد الخليجي في نصف العقد أو العقد القادم هو اتجاه حميد حيث ان تطور آليات عمل مجلس التعاون تتطلب التركيز على الآفاق الاستراتيجية لخلق اقتصاد خليجي موحد وعدم الاقتصار على التركيز على تعزيز التكامل الاقتصادي الخليجي أو تكامل الأنشطة الاقتصادية الخليجية ذلك ان نقارن بعين استراتيجيا بين خلق قاعدة للاندماج الاقتصادي الخليجي أو التكامل الاقتصادي الخليجي وما تتجه نحو جهود هذا المؤتمر هو تجاوز مرحلة التكامل والبدء بمناقشة متطلبات الاندماج والوحدة في سوق مشتركة الجهود الرسمية كافة ذات الصلة بالاقتصاد الخليجي قد تركزت على التكامل بين الأنشطة الاقتصادية الخليجي وعدم التنافس في انشاء الصناعات المتشابهة لذلك فان الصناعات البتروكيميائية وصناعة الألمونيوم والحديد والصلب تركزت عند دولة من الدول لكي لا تخلق حالة من التنافس الضار وبالطبع فان المنطق الذي حكم مثل ذلك التوجه يعد سليما وبخاصة فيما يتعلق بالصناعات الثقيلة ومن جانب آخر فإن التشريعات الاقتصادية الخليجية التي ترافقت مع التوقيع على الاتفاقية الاقتصادية الموحدة الأولى وبعدها الثانية أتاحت قدرا أكبر من حرية الحركة الاقتصادية لما هو معروف بالاستثمار المؤسسي لكن حركة هذا الاستثمار بقيت محكومة بالارادة الرسمية والمصالح الوطنية أو المحلية الصرفة ذات الصلة بفئات معينة نافذة هنا أو هناك  لكن حركة الافراد ضمن الاقتصاد لم تصل الى مستوى الحرية المطلوبة وفقا للفلسفة الاقتصادية الرأسمالية كما ان مستوى المنافسة لم يطلق حتى الآن المؤتمر الذي تنظمه جمعية الاقتصاديين البحرينية يبدو انه سيكون المنطلق لتمهيد الطريق لطرح مسألة اطلاق المبادرة الفردية والمنافسة ضمن جهد عام لصياغة أسس قيام اقتصاد موحد وسوق خليجية مشتركة فالتركيز يتوجه لأهمية تطوير الاقتصاد الخليجي كوحدة واحدة ولا يمكن التركير بخلق مثل هذا الاتجاه العام ما لم تطلق مبادرة رأس المال الخاص الفردي سواء أكان هذا شخصا طبيعيا أم اعتباريا ان الفلسفة الاقتصادية الرأسمالية انما تركز على حركة الفرد بوصفه مولد المنافسة والنمو فالفرد ضمن حركته الدؤوبة لتحسين وضعه التنافسي يبحث عن افضل السبل للتغلب على منافسة أقرانه وكذلك فان الفرد بوصفه صانع الأفكار الاقتصادية ومن يقتنص الفرص المتاحة يفتح آفاقا جديدة لنمو الاقتصاد واتساعه لذلك فان حركة الاقتصاد الرأسمالي هي التي أسست لظهور الحركة الليبرالية سياسيا فالحرية الاقتصادية بحاجة الى حرية حركة تمنحها له الحرية السياسية الممثلة في التشريعات المنظمة للحركة من دون ان يؤدي ذلك الى كبحها اما من حيث ايجابيات انعقاد المؤتمر الاقتصادي الأول بجهد من جمعية الاقتصاديين البحرينية هو التعبير عن رغبة المجتمع المدني الخليجي في لعب دور بناء في معالجة مشكلات بناء الاقتصاد الخليجي الموحد ولعل حرص جمعية الاقتصاديين البحرينية على الدعوة لعقد اجتماع تشاوري للجمعيات الخليجية الاقتصادية للوصول الى شكل من اشكال التنسيق والتنظيم الاقتصادي الخليجي هو خطوة في الاتجاه الصحيح ان تزايد الادوار الاستشارية والمساهمات النظرية للجمعيات الاقتصادية الخليجية سيكون عامل دعم للتطور الاقتصادي الخليجي الموحد فالتطبيق لابد له من نظرية ولعل خير داعم للتطبيق الخليجي الاقتصادي الموحد وجهات نظر خليجية موحدة                        '\n",
            "Remaining files in category 'Economy': 908\n",
            "\n",
            "---\n",
            "\n",
            "Category: International news\n",
            "Error reading the file in category 'International news': [Errno 2] No such file or directory: ' دبلوماسيون يستبعدون مغادرة الأجانب للسعودية بعد هجوم جدة      قلل العديد من الدبلوماسيين الغربيين من اهمية المخاوف بشأن احتمال عملية رحيل جماعية للاجانب المقيمين في المملكة العربية السعودية العربية اثر الهجوم الدامي الاول من نوعه الذي استهدف القنصلية الامريكية في جدة في غضون ذلك قالت صحف سعودية إن قائد هجوم القاعدة على القنصلية الأمريكية في جدة كان قد سجن بسبب أفكاره المتطرفة وعمل مع جماعة الأمر بالمعروف والنهي عن المنكر من جانبه استنكر مفتي عام السعودية الشيخ عبدالعزيز بن عبدالله آل الشيخ اقتحام القنصلية الامريكية ووصفه بالعمل المحرم ومن كبائر الذنوب   وقال باري بيتش المتحدث باسم السفارة البريطانية عايش الناس هنا الكثير من الهجمات وهم مطلعون على الوضع وقرروا البقاء واشار الى انه لا يوجد احصاء موثوق لعدد البريطانيين في المملكة لانهم لا يقومون كلهم بتسجيل انفسهم في السفارة غير انه قدر عددهم حاليا بما بين  الى  الفا مقابل ما بين  و الفا سنة  وكان الهجوم على القنصلية الامريكية في جدة يوم الاثنين  قد اوقع خمسة قتلى بين العاملين من غير الامريكيين في القنصلية بالإضافة الى اربعة من المهاجمين والاعتداء هو الاخير في سلسلة طويلة من الهجمات في المملكة منذ مايو  نسبت الى تنظيم القاعدة وأوقعت مائة قتيل ربعهم من الغربيين وكان السفير الامريكي في السعودية جيمس اوبرواتر قد تفقد يوم الثلاثاء المجمع الذي يؤوي القنصلية في جدة واعاد التأكيد على تعليمات وزارةالخارجية الامريكية التي تطلب من المواطنين مغادرة السعودية وقال في مؤتمر صحفي في جدة ان هذا الاجراء يبقى ساريا ونأمل ان نرى اليوم الذي يتطور فيه الوضع لنتمكن من سحب هذا الانذار من جانبها اعتبرت المتحدثة باسم السفارة الامريكية كارولا كالين ان القول بان هجوم يوم الاثنين يمكن ان يدفع الاجانب الى المغادرة يظل مجرد تخمين ورفضت في المقابل كشف عدد الامريكيين الموجودين حاليا في المملكة بعد ان كان عددهم حوالي  الفا سنة  وحذرت كافة الممثليات الدبلوماسية الغربية منذ بدء اعمال العنف السنة الماضية مواطنيها من القيام بأي رحلات غير ضرورية الى المملكة وفي الواقع يبدو ان عمليات المغادرة تشمل وخصوصا اقارب الاجانب في المملكة حيث يبقى المتعاقد الاجنبي مقيما لكنه يفضل ترحيل اسرته الى اماكن تعتبر اكثر امنا كالبحرين ودولة الامارات مثلا وقال اندرو تيرنر المتحدث باسم السفارة الكندية في الرياض هناك تراجع طفيف في عدد مواطنينا منذ السنة الماضية ولكنه يشمل بالخصوص اقارب الموظفين الاجانب واضاف هناك آخرون قرروا البقاء موضحا ان ما بين  الى  الاف كندي يعيشون في السعودية مقابل حوالي  الاف في سنة  واشار الى ان موجة الاعتداءات دفعت الكنديين الى التسجيل لدى سفارتهم في المملكة وبدوره قال دبلوماسي فرنسي ان عدد الفرنسيين في السعودية بقي مستقرا في حدود اكثر من اربعة الاف شخص واضاف ان عدد الشركات يبلغ  شركة وان حالات المغادرة ترتبط بانتهاء العقود من ناحية أخرى قالت المتحدثة باسم السفارة كارول كالين  ان السفارة في الرياض وقنصلية الظهران في شرق السعودية عاودتا فتح ابوابهما في حين ان قنصلية جدة لا تزال مغلقة من جهة ثانية قالت الصحف السعودية اليومية إن فايز الجهني كان قائد خليه القاعدة في مدينة جدة وكان الجهني واحدا من المهاجمين الاربعة الذين قتلوا في الهجوم واصيب خامس بجروح واعتقل وتعرفت السلطات السعودية على هوية ثلاثة من القتلى ولم يكن أيا منهم ضمن قائمة لابرز المتشددين المطلوبين في المملكة ونقلت صحيفة عكاظ السعودية عن مصادر مقربة من أسرة الجهني قولها إنه سجن مدة أربعة أشهر بسبب أفكاره المتطرفة وأطلق سراحه في أكتوبر عام  واختفى مدة ثلاثة اشهر بعد ذلك وقالت الصحيفة إن شائعات ترددت في ذلك الوقت عن أنه ذهب إلى مدينة الفلوجة العراقية واعلن جناح تنظيم القاعدة في السعودية مسؤوليته عن الهجوم على القنصلية الذي اطلق عليه غزوة الفلوجة المباركة على اسم المدينة العراقية التي شنت قوات تقودها الولايات المتحدة هجوما عليها لملاحقة مسلحين من بينهم أنصار المتشدد أبومصعب الزرقاوي حليف القاعدة ونقلت صحيفة الوطن السعودية عن ابن خال الجهني قوله إنه طرد من جماعة الأمر بالمعروف والنهي عن المنكر قبل خمس سنوات بسبب سوء سلوكه الوظيفي وقالت عكاظ إن الجهني الذي كان في منتصف العشرينيات من عمره قد انضم إلى جماعة الأمر بالمعروف بعد أن أنهى دراسته الثانوية ثم فصل بعد أن اعتدى على بعض المحتجزين لدى الجماعة من جانبه استنكر مفتي عام السعودية الشيخ عبدالعزيز بن عبدالله آل الشيخ اقتحام القنصلية الامريكية ووصفه بالعمل المحرم ومن كبائر الذنوب واخيرا قال مفتي عام السعودية الشيخ عبدالعزيز بن عبدالله آل الشيخ في بيان له ما حصل الاثنين الماضي من محاولة لاقتحام القنصلية الامريكية بجدة واستخدام الاسلحة والمتفجرات وقتل الانفس المعصومة وترويع الآمنين وزعزعة الامن في المملكة  كل هذا من المحرمات بل ومن كبائر الذنوب وأضاف كل من دخل بلادنا بإذن من ولاة الامر فانه قد أعطي الامان ولا يجوز التعرض له وقال المفتي قد تكرر الانكار والتشديد في بيان حرمة هذه الاعمال وشناعتها من قبل العلماء من داخل البلاد وخارجها ولا نعلم أن عالما معتبرا قد أجاز مثل هذه الاعمال                  '\n",
            "Remaining files in category 'International news': 952\n",
            "\n",
            "---\n",
            "\n",
            "Category: Local News\n",
            "Error reading the file in category 'Local News': [Errno 2] No such file or directory: ' في اقتراح للنائب جاسم الموالي جهاز لتحلية المياه في كل بيت تتحمل الحكومة تكلفته بالكامل   تقدم النائب جاسم الموالي باقتراح برغبة إلى مجلس النواب بان تقوم الحكومة بتحمل تكلفة تركيب جهاز تحلية وتنقية المياه في كل بيت يسكنه بحريني وطالبت المذكرة الإيضاحية التي تقدم بها النائب بأن تقوم الحكومة بتحمل تكلفة تركيب جهاز تحلية وتنقية الماء لكل منزل بالشروط الآتية   ان يكون ساكنو المنزل من المواطنين البحرينيين وألا يزيد دخل الاسرة الشهري عن خمسمائة دينار وألا يكون احد اعضاء الاسرة ممن يملكون سجلا تجاريا وتتحمل الحكومة التكلفة مرة واحدة ولا تتحمل الحكومة مصاريف الصيانة  ويوجد ضمان مدة سنة واحدة على الجهاز الحالي المطروح في الاسواق  ولا تكلفة تبديل المرشحات مىئ ولا تكلفة التركيب وانما يتحمل رب الاسرة هذه التكاليف وهي تكاليف زهيدة م على ان يتم التنسيق بين الجهات المختصة في الحكومة لتنفيذ هذا المشروع ومنها على سبيل المثال  لجنة المناقصات لجنة حماية البيئة وزارة الصحة وزارة المالية كما تقوم الحكومة بإيجاد وظائف للعاملين الحاليين في بيع المياه المحلاة  وعددهم ليس بالكبير  الذين سيتضررون جراء هذا المشروع والمتوقع ان تقل تكلفة هذا المشروع عن مليون ونصف  دينار  حيث يتوقع ان يقل عدد الاسر التي تنطبق عليها الشروط المذكورة عن ثلاثين ألف اسرة كما يتوقع ان تستطيع الحكومة ان تبتاع  محليا او من خارج المملكة  الجهاز المذكور بمبلغ يقل عن خمسين دينارا بحرينيا والجهاز متوافر حاليا بمبلغ خمسة وستين دينارا بحرينيا كسعر للتجزئة شاملا اجر التركيب والضمان لمدة عام وكذلك ارباح المزود التي ما أخالها تقل عن نسبة  من سعر البيع أما عن اعتبارات المصلحة العامة المبررة لعرض الاقتراح فيقول النائب تصل إلى كثير من الاسر حاليا مياه مالحة غير صالحة للشرب مما يضطرهم إلى شراء مياه نقية للشرب والطبخ ولتخفيف عبء المعيشة على المواطنين محدودي الدخل وللمساهمة في رفاهية المواطن الذي هو هدف من اهداف المملكة والمساهمة في الوقاية من الامراض كما سيشجع هذا الاجراء الاسر التي لا تنطبق عليها المواصفات على شراء اجهزة مماثلة لمنازلهم مما سيساهم في رفع مستوى الصحة العامة وسيعوض المزودين المحليين عن خسائرهم في هذا المجال فيما لو قررت الحكومة استيراد الجهاز من خارج المملكة وهو  اعنى الاخير  اجراء غير محبذ                    '\n",
            "Remaining files in category 'Local News': 2397\n",
            "\n",
            "---\n",
            "\n",
            "Category: Sports\n",
            "Error reading the file in category 'Sports': [Errno 2] No such file or directory: ' مسابقة كأس وزير الداخلية لكرة القدم   القوات الخاصة والتحقيقات الجنائية إلى دور النصف النهائي    بعد مباراة سريعة وقوية تأهل فريق الإدارة العامة للتحقيقات والمباحث الجنائية إلى دور نصف النهائي لمسابقة كأس وزير الداخلية بعد تغلبه على فريق كلية الشرطة الملكية بثلاثة أهداف مقابل هدف في ثاني مباريات دور الثمانية حيث بدأت المباراة سريعة من جانب الفريقين    ومن تمريرة جميلة من اللاعب طلال حسن يحرز عبدالقادر محمود الهدف الأول لفريق التحقيقات في الدقيقة  وبعدها بدقيقتين ومن تمريرة من المبدع طلال حسن يمر نايف الماجد من حارس فريق الكلية ويلعبها في المرمى مسجلا الهدف الثاني في الدقيقة  لينتهي الشوط الأول بتقدم التحقيقات صفر ومع بداية الشوط الثاني دخل فريق الكلية مهاجما ومن تسديدة من خارج منطقة الجزاء أحرز حمد راكع هدف الكلية الأول في الدقيقة  وفي الدقيقة احتسب حكم المباراة ضربة جزاء عندما عرقل حارس الكلية نضال عبدالحسين لاعب التحقيقات علي سعد نفذها طلال حسن محرزا هدف التحقيقات الثالث لتنتهي المباراة بفوز التحقيقات الجنائية على كلية الشرطة الملكية  وفي المباراة الأولى وضمن نفس المسابقة تأهل فريق إدارة قوات الأمن الخاصة بعد فوزه على فريق إدارة أمن المطار  حيث بدأ فريق الخاصة التسجيل عن طريق اللاعب محمد عبود في الدقيقة  من الشوط الأول عندما حول كرة برأسه على يسار حارس أمن المطار محمد علي وفي الدقيقة  تمكن اللاعب أحمد جاسم من تعديل النتيجة عندما لعب كرة برأسه لم يحسن عبدالله بلال حارس الخاصة في مسكها لتعبر خط المرمى معلنا هدف التعادل لفريق أمن المطار في الدقيقة  لينتهي الشوط الأول بتعادل الفريقين  وفي الدقيقة  من الشوط الثاني أحرز اللاعب هشام حمد عن طريق الخطأ هدف في مرمى فريقه عندما حاول إبعاد كرة لاعب الخاصة حسن الياسي معلنا الهدف الثاني لتنتهي المباراة بفوز الخاصة  وتأهله لملاقاة التحقيقات الجنائية في دور النصف النهائي يوم الثلاثاء القادم صرح بذلك النقيب خالد عبدالعزيز الخياط ضابط الاتحاد الرياضي للأمن العام                      '\n",
            "Remaining files in category 'Sports': 1429\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read a sample document from each category\n",
        "for category, files in categories.items():\n",
        "    if files:  # Check if there are files in the category\n",
        "        print(f\"Category: {category}\")\n",
        "        try:\n",
        "            # Open and read the first document\n",
        "            with open(files[0], 'r', encoding='utf-8') as f:\n",
        "                sample_content = f.read()[:500]  # Read the first 500 characters\n",
        "                print(\"Document Content (First 500 characters):\")\n",
        "                print(sample_content)\n",
        "                print(f\"Character count: {len(sample_content)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading the file in category '{category}': {e}\")\n",
        "\n",
        "        print(f\"Remaining files in category '{category}': {len(files) - 1}\")\n",
        "        print(\"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdiWP5b-SMCj"
      },
      "source": [
        "### Text Preprocessing, Feature Engineering, and TF-IDF Conversion\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "VGoBKomsM7bx",
        "outputId": "931ecf82-1c88-486f-ad1d-9e2d04b05993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset with Tokens and Crafted Features:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Text</th>\n",
              "      <th>Cleaned_Text</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Word_Count</th>\n",
              "      <th>Char_Count</th>\n",
              "      <th>Contains_Economy</th>\n",
              "      <th>Presence_البورصة</th>\n",
              "      <th>Presence_الاقتصاد</th>\n",
              "      <th>Presence_الدولار</th>\n",
              "      <th>...</th>\n",
              "      <th>Frequency_الرئيس</th>\n",
              "      <th>Frequency_الحكومة</th>\n",
              "      <th>Frequency_البرلمان</th>\n",
              "      <th>Frequency_مجلس الأمن</th>\n",
              "      <th>Frequency_الأمم</th>\n",
              "      <th>Frequency_الدولة</th>\n",
              "      <th>Frequency_السلطة</th>\n",
              "      <th>Frequency_السياسة</th>\n",
              "      <th>Frequency_القانون</th>\n",
              "      <th>Frequency_الانتخابات</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Economy</td>\n",
              "      <td>جمعية الاقتصاديين الدور المأمول   حسنا فعلت ...</td>\n",
              "      <td>جمعيه الاقتصاديين الدور المامول حسنا فعلت جمعي...</td>\n",
              "      <td>[جمعيه, الاقتصاديين, الدور, المامول, حسنا, فعل...</td>\n",
              "      <td>489</td>\n",
              "      <td>3088</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Economy</td>\n",
              "      <td>سهم المجموعة العربية للتأمين يرتفع قياسيا و...</td>\n",
              "      <td>سهم المجموعه العربيه للتامين يرتفع قياسيا ويسج...</td>\n",
              "      <td>[سهم, المجموعه, العربيه, للتامين, يرتفع, قياسي...</td>\n",
              "      <td>267</td>\n",
              "      <td>1617</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Economy</td>\n",
              "      <td>افتتاح مركز النقد لجيسيك آند ديفريانت في الب...</td>\n",
              "      <td>افتتاح مركز النقد لجيسيك اند ديفريانت في البحر...</td>\n",
              "      <td>[افتتاح, مركز, النقد, لجيسيك, اند, ديفريانت, ف...</td>\n",
              "      <td>579</td>\n",
              "      <td>3497</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Economy</td>\n",
              "      <td>أسعار النفط تتراجع في نوفمبر بعد ارتفاعاتها...</td>\n",
              "      <td>اسعار النفط تتراجع في نوفمبر بعد ارتفاعاتها ال...</td>\n",
              "      <td>[اسعار, النفط, تتراجع, في, نوفمبر, بعد, ارتفاع...</td>\n",
              "      <td>340</td>\n",
              "      <td>2008</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Economy</td>\n",
              "      <td>خلال الفترة بين  من الشهر الجاري  الاجتماع ال...</td>\n",
              "      <td>خلال الفتره بين من الشهر الجاري الاجتماع السنو...</td>\n",
              "      <td>[خلال, الفتره, بين, من, الشهر, الجاري, الاجتما...</td>\n",
              "      <td>197</td>\n",
              "      <td>1254</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 69 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  Category                                               Text  \\\n",
              "0  Economy    جمعية الاقتصاديين الدور المأمول   حسنا فعلت ...   \n",
              "1  Economy     سهم المجموعة العربية للتأمين يرتفع قياسيا و...   \n",
              "2  Economy    افتتاح مركز النقد لجيسيك آند ديفريانت في الب...   \n",
              "3  Economy     أسعار النفط تتراجع في نوفمبر بعد ارتفاعاتها...   \n",
              "4  Economy   خلال الفترة بين  من الشهر الجاري  الاجتماع ال...   \n",
              "\n",
              "                                        Cleaned_Text  \\\n",
              "0  جمعيه الاقتصاديين الدور المامول حسنا فعلت جمعي...   \n",
              "1  سهم المجموعه العربيه للتامين يرتفع قياسيا ويسج...   \n",
              "2  افتتاح مركز النقد لجيسيك اند ديفريانت في البحر...   \n",
              "3  اسعار النفط تتراجع في نوفمبر بعد ارتفاعاتها ال...   \n",
              "4  خلال الفتره بين من الشهر الجاري الاجتماع السنو...   \n",
              "\n",
              "                                              Tokens  Word_Count  Char_Count  \\\n",
              "0  [جمعيه, الاقتصاديين, الدور, المامول, حسنا, فعل...         489        3088   \n",
              "1  [سهم, المجموعه, العربيه, للتامين, يرتفع, قياسي...         267        1617   \n",
              "2  [افتتاح, مركز, النقد, لجيسيك, اند, ديفريانت, ف...         579        3497   \n",
              "3  [اسعار, النفط, تتراجع, في, نوفمبر, بعد, ارتفاع...         340        2008   \n",
              "4  [خلال, الفتره, بين, من, الشهر, الجاري, الاجتما...         197        1254   \n",
              "\n",
              "   Contains_Economy  Presence_البورصة  Presence_الاقتصاد  Presence_الدولار  \\\n",
              "0                 1                 0                  1                 0   \n",
              "1                 0                 0                  0                 0   \n",
              "2                 1                 0                  1                 1   \n",
              "3                 1                 0                  1                 0   \n",
              "4                 0                 0                  0                 0   \n",
              "\n",
              "   ...  Frequency_الرئيس  Frequency_الحكومة  Frequency_البرلمان  \\\n",
              "0  ...                 0                  0                   0   \n",
              "1  ...                 0                  0                   0   \n",
              "2  ...                 1                  0                   0   \n",
              "3  ...                 1                  0                   0   \n",
              "4  ...                 0                  0                   0   \n",
              "\n",
              "   Frequency_مجلس الأمن  Frequency_الأمم  Frequency_الدولة  Frequency_السلطة  \\\n",
              "0                     0                0                 0                 0   \n",
              "1                     0                0                 0                 0   \n",
              "2                     0                0                 0                 0   \n",
              "3                     0                0                 0                 0   \n",
              "4                     0                0                 0                 0   \n",
              "\n",
              "   Frequency_السياسة  Frequency_القانون  Frequency_الانتخابات  \n",
              "0                  0                  0                     0  \n",
              "1                  0                  0                     0  \n",
              "2                  0                  0                     0  \n",
              "3                  0                  0                     0  \n",
              "4                  0                  0                     0  \n",
              "\n",
              "[5 rows x 69 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Keywords Used: 31\n",
            "Keywords: ['البورصة', 'الاقتصاد', 'الدولار', 'الأسهم', 'السوق', 'التجارة', 'المال', 'الميزانية', 'النقد', 'الاستثمار', 'الإنتاج', 'الكرة', 'المباراة', 'الفريق', 'الدوري', 'الكأس', 'الأهداف', 'المنتخب', 'اللعبة', 'الرياضة', 'التنس', 'الرئيس', 'الحكومة', 'البرلمان', 'مجلس الأمن', 'الأمم', 'الدولة', 'السلطة', 'السياسة', 'القانون', 'الانتخابات']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "categories = {}\n",
        "\n",
        "# قراءة الملفات وتصنيفها حسب المجلد\n",
        "for folder in os.listdir(directory_path):\n",
        "    folder_path = os.path.join(directory_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        categories[folder] = []\n",
        "        for file_path in os.listdir(folder_path):\n",
        "            full_path = os.path.join(folder_path, file_path)\n",
        "            try:\n",
        "                if os.path.isfile(full_path):\n",
        "                    with open(full_path, 'r', encoding='utf-8') as f:\n",
        "                        categories[folder].append(f.read())\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file: {full_path} - {e}\")\n",
        "\n",
        "# تحويل النصوص إلى DataFrame\n",
        "data = [{\"Category\": category, \"Text\": text} for category, texts in categories.items() for text in texts]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# تنظيف النصوص\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'[إأآا]', 'ا', text)\n",
        "    text = re.sub(r'ى', 'ي', text)\n",
        "    text = re.sub(r'ؤ', 'و', text)\n",
        "    text = re.sub(r'ة', 'ه', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "df['Cleaned_Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Tokenize the cleaned text into words\n",
        "df['Tokens'] = df['Cleaned_Text'].apply(lambda x: x.split())\n",
        "\n",
        "# Calculate basic handcrafted features\n",
        "df['Word_Count'] = df['Tokens'].apply(len)  # Count the number of words\n",
        "df['Char_Count'] = df['Cleaned_Text'].apply(len)  # Count the number of characters\n",
        "df['Contains_Economy'] = df['Cleaned_Text'].apply(lambda x: 1 if 'اقتصاد' in x else 0)  # Check if \"Economy\" exists in text\n",
        "\n",
        "# List of keywords to use for additional features\n",
        "keywords = [\n",
        "    'البورصة', 'الاقتصاد', 'الدولار', 'الأسهم', 'السوق', 'التجارة', 'المال', 'الميزانية', 'النقد', 'الاستثمار',\n",
        "    'الإنتاج', 'الكرة', 'المباراة', 'الفريق', 'الدوري', 'الكأس', 'الأهداف', 'المنتخب', 'اللعبة', 'الرياضة',\n",
        "    'التنس', 'الرئيس', 'الحكومة', 'البرلمان', 'مجلس الأمن', 'الأمم', 'الدولة', 'السلطة', 'السياسة', 'القانون',\n",
        "    'الانتخابات', \n",
        "]\n",
        "\n",
        "# Create Presence and Frequency features for all keywords\n",
        "presence_features = pd.DataFrame({\n",
        "    f'Presence_{keyword}': df['Cleaned_Text'].str.contains(keyword).astype(int) for keyword in keywords\n",
        "})\n",
        "frequency_features = pd.DataFrame({\n",
        "    f'Frequency_{keyword}': df['Cleaned_Text'].apply(lambda x: x.count(keyword)) for keyword in keywords\n",
        "})\n",
        "\n",
        "# Add all new handcrafted features to the DataFrame\n",
        "df = pd.concat([df, presence_features, frequency_features], axis=1)\n",
        "\n",
        "# Extract TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=5000, min_df=2, max_df=0.85)\n",
        "tfidf_features = vectorizer.fit_transform(df['Cleaned_Text'])\n",
        "\n",
        "# Combine TF-IDF features with handcrafted features\n",
        "crafted_features = sp.csr_matrix(df[\n",
        "    ['Word_Count', 'Char_Count'] +\n",
        "    [f'Presence_{keyword}' for keyword in keywords] +\n",
        "    [f'Frequency_{keyword}' for keyword in keywords]\n",
        "].values)\n",
        "final_features = sp.hstack([tfidf_features, crafted_features])\n",
        "\n",
        "# عرض الإطار مع الميزات\n",
        "print(\"Dataset with Tokens and Crafted Features:\")\n",
        "display(df.head())\n",
        "\n",
        "# عرض الكلمات المستخدمة\n",
        "print(f\"Total Keywords Used: {len(keywords)}\")\n",
        "print(f\"Keywords: {keywords}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdJ7kKfQra4-"
      },
      "source": [
        "### Selected Top 20 Feature\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9nWRTRpUxnn",
        "outputId": "bb185931-f3a6-4fc9-fce5-cb666ccb3088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Top 20 Feature Indices: [ 155  279  383  506  592  833 1296 1298 1406 1443 1469 1549 1815 2276\n",
            " 2293 3414 3774 5000 5001 5002]\n",
            "Selected Top 20 Feature Scores: [1.62295172e+02 1.25897579e+02 1.48455513e+02 1.83086384e+02\n",
            " 1.75642401e+02 1.44931088e+02 1.63974486e+02 1.33690145e+02\n",
            " 1.76313719e+02 1.82270107e+02 1.28675740e+02 1.70958537e+02\n",
            " 1.76802308e+02 1.34469261e+02 1.68853516e+02 1.65953428e+02\n",
            " 1.75517557e+02 4.96019842e+04 2.78521135e+05 8.92468678e+02]\n",
            "Selected Feature Names: ['اسرائيل', 'الاتحاد', 'الاسرائيلي', 'الانتخابات', 'البطوله', 'الجيش', 'العراق', 'العراقيه', 'الفلسطينيه', 'القدم', 'القوات', 'المباراه', 'المنتخب', 'بطوله', 'بغداد', 'غزه', 'لكره', 'Word_Count', 'Char_Count', 'Contains_Economy']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Combine handcrafted features and TF-IDF features\n",
        "crafted_features = sp.csr_matrix(df[['Word_Count', 'Char_Count', 'Contains_Economy']].values)\n",
        "final_features = sp.hstack([tfidf_features, crafted_features])\n",
        "\n",
        "# Encode the target labels into numerical format\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "# Apply SelectKBest to select top 20 features\n",
        "selector = SelectKBest(score_func=chi2, k=20)\n",
        "selected_features = selector.fit_transform(final_features, y)\n",
        "\n",
        "# Get the scores and indices of the selected features\n",
        "feature_scores = selector.scores_\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Display the selected feature indices and scores\n",
        "print(\"Selected Top 20 Feature Indices:\", selected_indices)\n",
        "print(\"Selected Top 20 Feature Scores:\", feature_scores[selected_indices])\n",
        "\n",
        "# Display the names of the selected features\n",
        "feature_names = list(vectorizer.get_feature_names_out()) + ['Word_Count', 'Char_Count', 'Contains_Economy']\n",
        "selected_feature_names = [feature_names[i] for i in selected_indices]\n",
        "print(\"Selected Feature Names:\", selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9_nW8zuW37"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uJ_5ag9UVdpA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NzqtjiWuZHC"
      },
      "source": [
        "### Combine All Features (TF-IDF + Handcrafted Features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acF_moWfVfy9",
        "outputId": "2d1eb5d0-d6a4-429c-e991-56a9f046f2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Feature Names: ['اسرائيل', 'الاتحاد', 'الاسرائيلي', 'الانتخابات', 'البطوله', 'الجيش', 'العراق', 'العراقيه', 'الفلسطينيه', 'القدم', 'القوات', 'المباراه', 'المنتخب', 'بطوله', 'بغداد', 'غزه', 'لكره', 'Word_Count', 'Char_Count', 'Contains_Economy']\n"
          ]
        }
      ],
      "source": [
        "# Combine all feature names (TF-IDF + handcrafted features)\n",
        "feature_names = list(vectorizer.get_feature_names_out()) + ['Word_Count', 'Char_Count', 'Contains_Economy']\n",
        "\n",
        "# Extract all selected feature names\n",
        "selected_feature_names = [feature_names[i] for i in selected_indices]\n",
        "print(\"Selected Feature Names:\", selected_feature_names)\n",
        "\n",
        "# Combine TF-IDF features with handcrafted features\n",
        "crafted_features = sp.csr_matrix(df[['Word_Count', 'Char_Count', 'Contains_Economy']].values)\n",
        "final_features = sp.hstack([tfidf_features, crafted_features])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KClCEjWGulPO"
      },
      "source": [
        "### Select the Best Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUxlb5l0unW3",
        "outputId": "741beb25-a7df-4637-c732-609736b89617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Top 20 Feature Names: ['اسرائيل', 'الاتحاد', 'الاسرائيلي', 'الانتخابات', 'البطوله', 'الجيش', 'العراق', 'العراقيه', 'الفلسطينيه', 'القدم', 'القوات', 'المباراه', 'المنتخب', 'بطوله', 'بغداد', 'غزه', 'لكره', 'Word_Count', 'Char_Count', 'Contains_Economy']\n"
          ]
        }
      ],
      "source": [
        "# Use selected features for training\n",
        "selected_features = selector.fit_transform(final_features, y)\n",
        "\n",
        "# Display the selected feature names\n",
        "print(f\"Selected Top {len(selected_indices)} Feature Names:\", selected_feature_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJiQaJwTvN5p"
      },
      "source": [
        "### Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7eUhdguvQAL",
        "outputId": "405f864d-638c-45c5-a1d7-b1b35d41c445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training and testing data split successfully.\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(selected_features, y, test_size=0.2, random_state=42)\n",
        "print(\"Training and testing data split successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Vjd-9Vu2Db"
      },
      "source": [
        "### Convert the preprocessed data into a dense matrix for lazy classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s-yK-Ma0XRaB"
      },
      "outputs": [],
      "source": [
        "# Convert the preprocessed data into a dense matrix for lazy classifiers\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Combine TF-IDF and handcrafted features\n",
        "crafted_features = sp.csr_matrix(df[['Word_Count', 'Char_Count', 'Contains_Economy']].values)\n",
        "final_features = sp.hstack([tfidf_features, crafted_features]).toarray()  # Convert to dense\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['Category'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hcpWMr0KGaG"
      },
      "source": [
        "Train Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 30/31 [24:37<00:46, 46.41s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.123105 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 244904\n",
            "[LightGBM] [Info] Number of data points in the train set: 4552, number of used features: 4975\n",
            "[LightGBM] [Info] Start training from score -1.863707\n",
            "[LightGBM] [Info] Start training from score -1.790004\n",
            "[LightGBM] [Info] Start training from score -0.854426\n",
            "[LightGBM] [Info] Start training from score -1.376675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 31/31 [25:08<00:00, 48.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Performance Summary:\n",
            "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
            "Model                                                                          \n",
            "LogisticRegression                 0.94               0.94    None      0.94   \n",
            "LGBMClassifier                     0.94               0.94    None      0.94   \n",
            "XGBClassifier                      0.94               0.93    None      0.94   \n",
            "PassiveAggressiveClassifier        0.93               0.93    None      0.93   \n",
            "NearestCentroid                    0.93               0.93    None      0.93   \n",
            "Perceptron                         0.93               0.93    None      0.93   \n",
            "ExtraTreesClassifier               0.93               0.93    None      0.93   \n",
            "BernoulliNB                        0.92               0.92    None      0.92   \n",
            "LinearSVC                          0.92               0.92    None      0.92   \n",
            "CalibratedClassifierCV             0.93               0.92    None      0.93   \n",
            "RandomForestClassifier             0.92               0.91    None      0.92   \n",
            "SGDClassifier                      0.91               0.89    None      0.91   \n",
            "BaggingClassifier                  0.90               0.89    None      0.90   \n",
            "SVC                                0.91               0.88    None      0.91   \n",
            "GaussianNB                         0.86               0.86    None      0.86   \n",
            "DecisionTreeClassifier             0.84               0.82    None      0.84   \n",
            "AdaBoostClassifier                 0.81               0.78    None      0.80   \n",
            "RidgeClassifierCV                  0.75               0.75    None      0.75   \n",
            "ExtraTreeClassifier                0.76               0.75    None      0.76   \n",
            "NuSVC                              0.79               0.71    None      0.78   \n",
            "RidgeClassifier                    0.67               0.67    None      0.67   \n",
            "KNeighborsClassifier               0.61               0.62    None      0.62   \n",
            "LinearDiscriminantAnalysis         0.61               0.62    None      0.61   \n",
            "QuadraticDiscriminantAnalysis      0.22               0.25    None      0.14   \n",
            "LabelPropagation                   0.18               0.25    None      0.06   \n",
            "LabelSpreading                     0.18               0.25    None      0.06   \n",
            "DummyClassifier                    0.41               0.25    None      0.23   \n",
            "\n",
            "                               Time Taken  \n",
            "Model                                      \n",
            "LogisticRegression                   5.38  \n",
            "LGBMClassifier                      31.33  \n",
            "XGBClassifier                       87.71  \n",
            "PassiveAggressiveClassifier          5.83  \n",
            "NearestCentroid                      1.74  \n",
            "Perceptron                           4.70  \n",
            "ExtraTreesClassifier                33.90  \n",
            "BernoulliNB                          2.01  \n",
            "LinearSVC                          106.71  \n",
            "CalibratedClassifierCV             280.09  \n",
            "RandomForestClassifier              19.04  \n",
            "SGDClassifier                        7.10  \n",
            "BaggingClassifier                  129.71  \n",
            "SVC                                139.85  \n",
            "GaussianNB                           2.43  \n",
            "DecisionTreeClassifier              19.29  \n",
            "AdaBoostClassifier                  60.31  \n",
            "RidgeClassifierCV                   71.22  \n",
            "ExtraTreeClassifier                  1.96  \n",
            "NuSVC                              139.35  \n",
            "RidgeClassifier                      7.21  \n",
            "KNeighborsClassifier                 2.97  \n",
            "LinearDiscriminantAnalysis         168.16  \n",
            "QuadraticDiscriminantAnalysis      163.98  \n",
            "LabelPropagation                     6.54  \n",
            "LabelSpreading                       6.80  \n",
            "DummyClassifier                      1.58  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from lazypredict.Supervised import LazyClassifier\n",
        "\n",
        "\n",
        "X_train_dense = X_train  \n",
        "X_test_dense = X_test\n",
        "\n",
        "# Initialize LazyClassifier\n",
        "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
        "\n",
        "# Train and evaluate models\n",
        "models, predictions = clf.fit(X_train_dense, X_test_dense, y_train, y_test)\n",
        "\n",
        "# Display results\n",
        "print(\"Model Performance Summary:\")\n",
        "print(models)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Naive Bayes...\n",
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 0.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.71      0.79       203\n",
            "           1       0.95      0.96      0.95       193\n",
            "           2       0.86      0.94      0.90       461\n",
            "           3       0.99      0.97      0.98       281\n",
            "\n",
            "    accuracy                           0.91      1138\n",
            "   macro avg       0.92      0.90      0.90      1138\n",
            "weighted avg       0.91      0.91      0.91      1138\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Training LGBM Classifier...\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.137979 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 240006\n",
            "[LightGBM] [Info] Number of data points in the train set: 4552, number of used features: 4975\n",
            "[LightGBM] [Info] Start training from score -1.863707\n",
            "[LightGBM] [Info] Start training from score -1.790004\n",
            "[LightGBM] [Info] Start training from score -0.854426\n",
            "[LightGBM] [Info] Start training from score -1.376675\n",
            "\n",
            "LGBM Classifier Results:\n",
            "Accuracy: 0.94\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.88       203\n",
            "           1       0.97      0.96      0.97       193\n",
            "           2       0.92      0.95      0.93       461\n",
            "           3       0.98      0.99      0.98       281\n",
            "\n",
            "    accuracy                           0.94      1138\n",
            "   macro avg       0.95      0.93      0.94      1138\n",
            "weighted avg       0.94      0.94      0.94      1138\n",
            "\n",
            "\n",
            "---\n",
            "\n",
            "Training XGB Classifier...\n",
            "\n",
            "XGB Classifier Results:\n",
            "Accuracy: 0.94\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.84      0.88       203\n",
            "           1       0.96      0.96      0.96       193\n",
            "           2       0.92      0.96      0.94       461\n",
            "           3       0.98      0.98      0.98       281\n",
            "\n",
            "    accuracy                           0.94      1138\n",
            "   macro avg       0.95      0.93      0.94      1138\n",
            "weighted avg       0.94      0.94      0.94      1138\n",
            "\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Encode labels if necessary\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_features, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "classifiers = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"LGBM Classifier\": LGBMClassifier(),\n",
        "    \"XGB Classifier\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "}\n",
        "\n",
        "# Ensure that the label_encoder.classes_ are strings\n",
        "class_names = [str(label) for label in label_encoder.classes_]\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "results = {}\n",
        "for name, model in classifiers.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=class_names)\n",
        "\n",
        "    # Save results\n",
        "    results[name] = {\"Accuracy\": accuracy, \"Report\": report}\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\n{name} Results:\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(report)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hy0ExgYIjMM"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltOD7NtWMwvg",
        "outputId": "0276ccc6-9ddd-45b2-b2cc-0b1b234e08da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article 1: قالت الحكومة إن الاقتصاد الوطني شهد نموًا كبيرًا في السنوات الأخيرة.\n",
            "Predicted Category (Naive Bayes): Economy\n",
            "Predicted Category (LGBM Classifier): Local News\n",
            "Predicted Category (XGB Classifier): Local News\n",
            "\n",
            "---\n",
            "\n",
            "Article 2: حقق المنتخب الوطني فوزًا مذهلًا في المباراة النهائية لكأس البطولة.\n",
            "Predicted Category (Naive Bayes): Sports\n",
            "Predicted Category (LGBM Classifier): Sports\n",
            "Predicted Category (XGB Classifier): Sports\n",
            "\n",
            "---\n",
            "\n",
            "Article 3: أعلنت الأمم المتحدة عن مبادرات جديدة لدعم الأمن والسلام الدولي.\n",
            "Predicted Category (Naive Bayes): International news\n",
            "Predicted Category (LGBM Classifier): Local News\n",
            "Predicted Category (XGB Classifier): Local News\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# New articles\n",
        "new_articles = [\n",
        "    \"قالت الحكومة إن الاقتصاد الوطني شهد نموًا كبيرًا في السنوات الأخيرة.\",\n",
        "    \"حقق المنتخب الوطني فوزًا مذهلًا في المباراة النهائية لكأس البطولة.\",\n",
        "    \"أعلنت الأمم المتحدة عن مبادرات جديدة لدعم الأمن والسلام الدولي.\"\n",
        "]\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
        "    text = re.sub(r'[إأآا]', 'ا', text)\n",
        "    text = re.sub(r'ى', 'ي', text)\n",
        "    text = re.sub(r'ؤ', 'و', text)\n",
        "    text = re.sub(r'ة', 'ه', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Clean new articles\n",
        "cleaned_articles = [clean_text(article) for article in new_articles]\n",
        "\n",
        "# Transform new articles into features\n",
        "# TF-IDF features\n",
        "tfidf_features_new = vectorizer.transform(cleaned_articles)\n",
        "\n",
        "# Handcrafted features\n",
        "new_handcrafted_features = np.array([\n",
        "    [len(article.split()), len(article), 1 if 'اقتصاد' in article else 0]\n",
        "    for article in cleaned_articles\n",
        "])\n",
        "\n",
        "# Combine TF-IDF and handcrafted features\n",
        "final_features_new = sp.hstack([tfidf_features_new, sp.csr_matrix(new_handcrafted_features)]).toarray()\n",
        "# Predict using all models\n",
        "predictions = {}\n",
        "for name, model in classifiers.items():\n",
        "    model_predictions = model.predict(final_features_new)\n",
        "    # Decode numerical predictions back to string categories\n",
        "    predicted_categories = label_encoder.inverse_transform(model_predictions)\n",
        "    predictions[name] = predicted_categories\n",
        "\n",
        "# Display results\n",
        "for i, article in enumerate(new_articles):\n",
        "    print(f\"Article {i + 1}: {article}\")\n",
        "    for name, predicted_categories in predictions.items():\n",
        "        print(f\"Predicted Category ({name}): {predicted_categories[i]}\")\n",
        "    print(\"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvRoQiYpxLj0",
        "outputId": "b00375f3-a684-4015-e701-bd495f10129e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask) (8.1.7)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2024.12.14)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "! pip install flask-ngrok flask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmk_y2-yxOW9",
        "outputId": "51d5dbe8-cbe5-4575-c2f7-b35cdf217951"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['label_encoder.pkl']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "# Get the trained Naive Bayes model from the 'classifiers' dictionary\n",
        "naive_bayes_model = classifiers[\"Naive Bayes\"]\n",
        "\n",
        "# Save the trained model, vectorizer, and label encoder\n",
        "joblib.dump(naive_bayes_model, \"naive_bayes_model.pkl\")\n",
        "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
        "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
